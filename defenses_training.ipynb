{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from models import create_dynamic_neural_network, create_dynamic_dm_neural_network\n",
    "from train import train_adversarial\n",
    "from plotting import plot_training_metrics\n",
    "from dataloader_ids import load_and_prepare_data\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADVERSARIAL_TRAINING_MODES = ['BASELINE', 'PGD', 'MART', 'TRADES'] \n",
    "\n",
    "DATASETS = ['mirai', 'unsw-nb15']\n",
    "MULTICLASS = [False, True]\n",
    "ENCODINGS = ['DM', 'Stats', 'Raw']\n",
    "\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_PARAMS = {\n",
    "    \"mirai\": {\n",
    "        \"dm\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "        \"raw\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "    },\n",
    "    \"unsw-nb15\": {\n",
    "        \"dm\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "        \"raw\": {\n",
    "            \"pgd\": {\"eps\": 0.3, \"alpha\": 0.01, \"iters\": 40},\n",
    "            \"mart\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "            \"trades\": {\"step_size\": 0.003, \"epsilon\": 0.03, \"perturb_steps\": 10, \"beta\": 6.0},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_method_params(method, dataset, encoding):\n",
    "    method = method.lower()\n",
    "    dataset = dataset.lower()\n",
    "    encoding = encoding.lower()\n",
    "    \n",
    "    if dataset not in MODE_PARAMS:\n",
    "        raise ValueError(f\"Dataset '{dataset}' is not configured in MODE_PARAMS.\")\n",
    "    if encoding not in MODE_PARAMS[dataset]:\n",
    "        raise ValueError(f\"Encoding '{encoding}' is not configured for dataset '{dataset}'.\")\n",
    "    if method not in MODE_PARAMS[dataset][encoding]:\n",
    "        raise ValueError(f\"Mode '{method}' is not configured for dataset '{dataset}' and encoding '{encoding}'.\")\n",
    "    \n",
    "    return MODE_PARAMS[dataset][encoding][method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(dataset, encoding, method, constrain, model_timestamp, model):\n",
    "    if encoding in ['DM', 'Stats']:\n",
    "        model_save_path = f'trained_models/{dataset}/{encoding}/{method}_{constrain}/'\n",
    "    else:\n",
    "        model_save_path = f'trained_models/{dataset}/{encoding}/{method}/'\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_save_path + f'{model_timestamp}.pth')\n",
    "\n",
    "def save_training_results(dataset, encoding, method, constrain, model_timestamp, training_results):\n",
    "    if encoding in ['DM', 'Stats']:\n",
    "        results_save_path = f'results/training/{dataset}/{encoding}/{method}_{constrain}/'\n",
    "    else:\n",
    "        results_save_path = f'results/training/{dataset}/{encoding}/{method}/'\n",
    "    os.makedirs(os.path.dirname(results_save_path), exist_ok=True)\n",
    "    with open(results_save_path + f'{model_timestamp}.json', 'w') as f:\n",
    "        json.dump(training_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_dims(params):\n",
    "    hidden_dims = []\n",
    "    for key, value in params.items():\n",
    "        if key.startswith(\"hidden_dim_layer_\"):\n",
    "            hidden_dims.append(value)\n",
    "\n",
    "    # Sort by layer index in case keys are unordered\n",
    "    hidden_dims = [v for k, v in sorted((key, value) for key, value in params.items() if key.startswith(\"hidden_dim_layer_\"))]\n",
    "    return hidden_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = (\n",
    "    len(DATASETS) * len(ADVERSARIAL_TRAINING_MODES) *  # Base factors\n",
    "    (\n",
    "        len([enc for enc in ENCODINGS if enc in ['DM', 'Stats']]) * 2 +  # Encodings with constraints\n",
    "        len([enc for enc in ENCODINGS if enc not in ['DM', 'Stats']]) * 1  # Encodings without constraints\n",
    "    )\n",
    ")\n",
    "iteration_counter = 0 \n",
    "\n",
    "\n",
    "for dataset, multiclass in zip(DATASETS, MULTICLASS):\n",
    "    for encoding in ENCODINGS:\n",
    "        # Determine whether to include the 'constrain' loop\n",
    "        constrain_values = [True, False] if encoding in ['DM', 'Stats'] else [False]\n",
    "\n",
    "        print(f'DATASET: {dataset}, encoding: {encoding}')\n",
    "        dataset = dataset.lower()\n",
    "        \n",
    "        best_params_fp = f\"results/model_discovery/{dataset}/{encoding}/best_params.json\"\n",
    "        with open(best_params_fp, \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "            hidden_dims = extract_hidden_dims(best_params)\n",
    "            learning_rate = best_params[\"lr\"]\n",
    "            dropout_rate = best_params[\"dropout\"]\n",
    "            batch_size = best_params[\"batch_size\"]\n",
    "\n",
    "        train_loader, val_loader, _, _, input_dim, output_dim, y_mapping, _ = load_and_prepare_data(\n",
    "            dataset_key=dataset, \n",
    "            encoding_key=encoding,\n",
    "            multiclass=multiclass,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"Data loaded and DataLoader created successfully!\")\n",
    "        print(f'Input dimension: {input_dim}, Output dimension: {output_dim}')\n",
    "        print(f'Shapes: Train: {len(train_loader.dataset)}, Test: {len(val_loader.dataset)}')\n",
    "        \n",
    "        for constrain in constrain_values:\n",
    "            for method in ADVERSARIAL_TRAINING_MODES:\n",
    "                iteration_counter += 1\n",
    "            \n",
    "                method = method.lower()\n",
    "                method_params = {}\n",
    "                if method != 'baseline':\n",
    "                    method_params = get_method_params(method, dataset, encoding)\n",
    "\n",
    "                print(f' ------ [{iteration_counter}/{total_iterations}]: ADVERSARIAL TRAINING MODE: {method}, Constrain: {constrain}')\n",
    "                \n",
    "                if encoding == 'DM':\n",
    "                    model, criterion, optimizer = create_dynamic_dm_neural_network(\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim,\n",
    "                        multiclass=multiclass,\n",
    "                        hidden_dims=hidden_dims,\n",
    "                        optimizer=\"adam\",\n",
    "                        lr=learning_rate,\n",
    "                        dropout_rate=dropout_rate\n",
    "                    )\n",
    "                else:\n",
    "                    model, criterion, optimizer = create_dynamic_neural_network(\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim,\n",
    "                        multiclass=multiclass,\n",
    "                        hidden_dims=hidden_dims,\n",
    "                        optimizer=\"adam\",\n",
    "                        lr=learning_rate,\n",
    "                        dropout_rate=dropout_rate\n",
    "                    )\n",
    "\n",
    "                model = model.to(device)\n",
    "                print(model)\n",
    "                \n",
    "                training_results, model = train_adversarial(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion,\n",
    "                    device=device,\n",
    "                    encoding=encoding,\n",
    "                    constrain=constrain,\n",
    "                    num_epochs=NUM_EPOCHS,\n",
    "                    patience=25,\n",
    "                    method=method,\n",
    "                    verbose=False,\n",
    "                    **method_params\n",
    "                )\n",
    "                \n",
    "                model_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                save_model(dataset, encoding, method, constrain, model_timestamp, model)\n",
    "                save_training_results(dataset, encoding, method, constrain, model_timestamp, training_results)\n",
    "                plot_training_metrics(dataset, encoding, method, constrain, model_timestamp, training_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
