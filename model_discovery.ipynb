{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from models import create_dynamic_neural_network\n",
    "from train import train_adversarial, compute_validation_metrics\n",
    "import dataloader_ids\n",
    "from dataloader_ids import load_and_prepare_data\n",
    "\n",
    "import optuna\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "def objective(trial, dataset_key, encoding_key, multiclass=False, verbose=False):\n",
    "\n",
    "    suggested_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    if encoding_key == 'Raw':\n",
    "        if suggested_batch_size != 256:\n",
    "            suggested_batch_size = 256 \n",
    "             \n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 4)\n",
    "    hidden_dims = [\n",
    "        trial.suggest_int(f\"hidden_dim_layer_{i+1}\", 16, 128, step=16)\n",
    "        for i in range(num_layers)\n",
    "    ]\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    print(f\"Trial: {trial.number}, Batch Size: {suggested_batch_size}, Hidden Dims: {hidden_dims}, Dropout: {dropout_rate}, LR: {learning_rate}\")\n",
    "\n",
    "    ###### IDS ######\n",
    "    train_loader, _, test_loader, _, input_dim, output_dim, _, _, _ = load_and_prepare_data(\n",
    "        dataset_key=dataset_key, \n",
    "        encoding_key=encoding_key, \n",
    "        multiclass=multiclass, \n",
    "        batch_size=suggested_batch_size, \n",
    "        model_discovery=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(train_loader.dataset)}, Batch size: {suggested_batch_size}, Num batches: {len(train_loader)}\")\n",
    "\n",
    "    train_size = len(train_loader.dataset)\n",
    "    if train_size < suggested_batch_size:\n",
    "        adjusted_batch_size = max(16, train_size // 10)  # Default to at least 10 batches if possible\n",
    "        print(f\"Adjusted batch size to {adjusted_batch_size} due to small dataset size.\")\n",
    "        train_loader, _, test_loader, _, input_dim, output_dim, _, _, _ = load_and_prepare_data(\n",
    "            dataset_key=dataset_key, \n",
    "            encoding_key=encoding_key, \n",
    "            multiclass=multiclass, \n",
    "            batch_size=adjusted_batch_size, \n",
    "            model_discovery=True\n",
    "        )\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model, criterion, optimizer = create_dynamic_neural_network(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        multiclass=multiclass,\n",
    "        hidden_dims=hidden_dims,\n",
    "        optimizer=\"adam\",\n",
    "        lr=learning_rate,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    results, model = train_adversarial(\n",
    "        model, train_loader, test_loader, optimizer, criterion, device, num_epochs=NUM_EPOCHS, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Extract metrics\n",
    "    val_loss = results[\"val_losses\"][-1]\n",
    "    val_acc = results[\"val_accuracies\"][-1]\n",
    "    val_precision = results[\"val_precisions\"][-1]\n",
    "    val_recall = results[\"val_recalls\"][-1]\n",
    "    val_f1 = results[\"val_f1s\"][-1]\n",
    "\n",
    "    # Store additional metrics in the trial\n",
    "    trial.set_user_attr(\"val_acc\", val_acc)\n",
    "    trial.set_user_attr(\"val_f1\", val_f1)\n",
    "    trial.set_user_attr(\"val_precision\", val_precision)\n",
    "    trial.set_user_attr(\"val_recall\", val_recall)\n",
    "\n",
    "    print(f\"Trial: {trial.number}, Val Loss: {val_loss}, Val Acc: {val_acc}, Val Precision: {val_precision}, Val Recall: {val_recall}, Val F1: {val_f1}\")\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_trial(study, dataset_key, encoding_key, multiclass, verbose):\n",
    "    seed_params = {\n",
    "        \"num_layers\": 2,\n",
    "        \"hidden_dim_layer_1\": 64,\n",
    "        \"hidden_dim_layer_2\": 32,\n",
    "        \"dropout\": 0.0,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 256 if encoding_key in [\"Raw\", \"Flows\"] else 128\n",
    "    }\n",
    "\n",
    "    distributions = {\n",
    "        \"batch_size\": optuna.distributions.CategoricalDistribution([64, 128, 256]),\n",
    "        \"num_layers\": optuna.distributions.IntDistribution(2, 4),\n",
    "        \"hidden_dim_layer_1\": optuna.distributions.IntDistribution(16, 128, step=16),\n",
    "        \"hidden_dim_layer_2\": optuna.distributions.IntDistribution(16, 128, step=16),\n",
    "        \"dropout\": optuna.distributions.FloatDistribution(0.0, 0.5),\n",
    "        \"lr\": optuna.distributions.FloatDistribution(1e-5, 1e-1, log=True)\n",
    "    }\n",
    "\n",
    "    trial = optuna.trial.FrozenTrial(\n",
    "        number=len(study.trials),\n",
    "        trial_id=len(study.trials),\n",
    "        state=optuna.trial.TrialState.COMPLETE,\n",
    "        value=None,\n",
    "        values=None,\n",
    "        datetime_start=None,\n",
    "        datetime_complete=None,\n",
    "        params=seed_params,\n",
    "        distributions=distributions,\n",
    "        user_attrs={},\n",
    "        system_attrs={},\n",
    "        intermediate_values={},\n",
    "    )\n",
    "\n",
    "    # Evaluate the objective with seed parameters\n",
    "    val_loss = objective(trial, dataset_key, encoding_key, multiclass, verbose)\n",
    "\n",
    "    # Add the trial to the study\n",
    "    study.add_trial(\n",
    "        optuna.trial.FrozenTrial(\n",
    "            number=len(study.trials),\n",
    "            trial_id=len(study.trials),\n",
    "            state=optuna.trial.TrialState.COMPLETE,\n",
    "            value=val_loss,\n",
    "            values=None,\n",
    "            datetime_start=datetime.now(),\n",
    "            datetime_complete=datetime.now(),\n",
    "            params=seed_params,\n",
    "            distributions=distributions,\n",
    "            user_attrs={\n",
    "                \"seed_trial\": True,\n",
    "                \"val_acc\": trial.user_attrs[\"val_acc\"],\n",
    "                \"val_f1\": trial.user_attrs[\"val_f1\"],\n",
    "                \"val_precision\": trial.user_attrs[\"val_precision\"],\n",
    "                \"val_recall\": trial.user_attrs[\"val_recall\"]\n",
    "            },  # Mark as seed trial\n",
    "            system_attrs={},\n",
    "            intermediate_values={},\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(study, threshold=0.001, patience=10):\n",
    "    \"\"\"\n",
    "    Check if optimization is converging.\n",
    "    \n",
    "    Args:\n",
    "        study (optuna.Study): The Optuna study object.\n",
    "        threshold (float): Minimum improvement to consider as progress.\n",
    "        patience (int): Number of trials to check for improvement.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if optimization has converged, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the best trial values\n",
    "    trials = study.trials\n",
    "    if len(trials) < patience + 1:\n",
    "        return False  # Not enough trials to check convergence\n",
    "\n",
    "    # Calculate improvement over the last `patience` trials\n",
    "    recent_losses = [t.value for t in trials[-patience - 1:]]\n",
    "    improvement = abs(recent_losses[-1] - min(recent_losses[:-1]))\n",
    "\n",
    "    return improvement < threshold\n",
    "\n",
    "\n",
    "def write_all_trials(study, dataset, encoding):\n",
    "    \"\"\"\n",
    "    Write all trials to a file, including the best trial index.\n",
    "    \n",
    "    Args:\n",
    "        study (optuna.Study): The Optuna study object.\n",
    "        dataset (str): Dataset name.\n",
    "        encoding (str): Encoding type.\n",
    "    \"\"\"\n",
    "    trial_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_data.append({\n",
    "            \"trial_number\": trial.number,\n",
    "            \"params\": trial.params,\n",
    "            \"value\": trial.value,\n",
    "            \"user_attrs\": trial.user_attrs,\n",
    "            \"duration\": str(trial.duration)\n",
    "        })\n",
    "    \n",
    "    # Write all trial data\n",
    "    base_dir = \"results/model_discovery\"\n",
    "\n",
    "    os.makedirs(f\"{base_dir}/{dataset}/{encoding}/\", exist_ok=True)\n",
    "    with open(f\"{base_dir}/{dataset}/{encoding}/all_trials.json\", \"w\") as f:\n",
    "        json.dump(trial_data, f, indent=4)\n",
    "    \n",
    "    # Write the index of the best trial\n",
    "    best_trial_index = study.best_trial.number\n",
    "    with open(f\"{base_dir}/{dataset}/{encoding}/best_trial_index.json\", \"w\") as f:\n",
    "        json.dump({\"best_trial_index\": best_trial_index}, f)\n",
    "\n",
    "def optimize_with_convergence(dataset, encoding, multiclass, max_trials=50, threshold=0.001, patience=10, verbose=False):\n",
    "    print(f\"Starting optimization for dataset: {dataset}, encoding: {encoding}\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    seed_trial(study, dataset_key=dataset.lower(), encoding_key=encoding, multiclass=multiclass, verbose=verbose)\n",
    "    \n",
    "    no_improvement_trials = 0\n",
    "    best_loss_so_far = float('inf')\n",
    "\n",
    "    for trial_idx in range(max_trials - 1):\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, dataset_key=dataset.lower(), encoding_key=encoding, multiclass=multiclass, verbose=verbose),\n",
    "            n_trials=1,\n",
    "            callbacks=[\n",
    "                lambda study, trial: print(\n",
    "                    f\"Trial {trial.number}/{max_trials} finished in {trial.duration}. Loss: {trial.value:.4f}\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        best_loss_in_study = study.best_value\n",
    "        if best_loss_in_study < best_loss_so_far - threshold:\n",
    "            best_loss_so_far = best_loss_in_study\n",
    "            no_improvement_trials = 0\n",
    "        else:\n",
    "            no_improvement_trials += 1\n",
    "\n",
    "        if no_improvement_trials >= patience:\n",
    "            print(f\"Early stopping triggered: No improvement in the last {patience} trials.\")\n",
    "            break\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(\"\\nOptimization completed.\")\n",
    "    print(f\"Best trial number: {best_trial.number}\")\n",
    "    print(f\"Best hyperparameters: {best_trial.params}\")\n",
    "    print(f\"Best validation loss: {best_trial.value}\")\n",
    "    print(f\"Best accuracy: {best_trial.user_attrs.get('val_acc')}\")\n",
    "    print(f\"Best F1 score: {best_trial.user_attrs.get('val_f1')}\")\n",
    "\n",
    "    # Write all trials and best trial index to files\n",
    "    write_all_trials(study, dataset, encoding)\n",
    "\n",
    "    return best_trial.params, best_trial.user_attrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['mirai', 'unsw-nb15']\n",
    "MULTICLASS = [False, True]\n",
    "\n",
    "ENCODINGS = ['Raw', 'DM', 'Stats']\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "MAX_TRIALS = 20\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "MIN_DELTA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(dataloader_ids)\n",
    "from dataloader_ids import load_and_prepare_data\n",
    "\n",
    "\n",
    "for multiclass, dataset in zip(MULTICLASS, DATASETS):\n",
    "    for encoding in ENCODINGS:\n",
    "        best_params, best_metrics = optimize_with_convergence(\n",
    "            dataset=dataset,\n",
    "            encoding=encoding,\n",
    "            multiclass=multiclass,\n",
    "            max_trials=MAX_TRIALS,\n",
    "            threshold=MIN_DELTA,\n",
    "            patience=EARLY_STOP_PATIENCE,\n",
    "            verbose=True        \n",
    "        )\n",
    "        base_dir = \"results/model_discovery\"\n",
    "        os.makedirs(f\"{base_dir}/{dataset}/{encoding}/\", exist_ok=True)\n",
    "        with open(f\"{base_dir}/{dataset}/{encoding}/best_params.json\", \"w\") as f:\n",
    "            json.dump(best_params, f)\n",
    "        with open(f\"{base_dir}/{dataset}/{encoding}/best_metrics.json\", \"w\") as f:\n",
    "            json.dump(best_metrics, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV ASSESSMENT OF BEST PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_dims(params):\n",
    "    hidden_dims = []\n",
    "    for key, value in params.items():\n",
    "        if key.startswith(\"hidden_dim_layer_\"):\n",
    "            hidden_dims.append(value)\n",
    "\n",
    "    # Sort by layer index in case keys are unordered\n",
    "    hidden_dims = [v for k, v in sorted((key, value) for key, value in params.items() if key.startswith(\"hidden_dim_layer_\"))]\n",
    "    return hidden_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(dataloader_ids)\n",
    "from dataloader_ids import load_and_prepare_data\n",
    "\n",
    "\n",
    "for multiclass, dataset in zip(MULTICLASS, DATASETS):\n",
    "    for encoding in ENCODINGS:\n",
    "        print(f\"Running stratified CV for dataset: {dataset}\")\n",
    "\n",
    "        results_dir = f\"results/model_discovery/{dataset}/{encoding}/\"\n",
    "        best_params_fp = f\"{results_dir}best_params.json\"\n",
    "        best_metrics_fp = f\"{results_dir}best_metrics.json\"\n",
    "        fold_metrics_fp = f\"{results_dir}fold_metrics.json\"\n",
    "        avg_metrics_fp = f\"{results_dir}avg_metrics.json\"\n",
    "\n",
    "        # Load best params and metrics\n",
    "        with open(best_params_fp, \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "            hidden_dims = extract_hidden_dims(best_params)\n",
    "            learning_rate = best_params[\"lr\"]\n",
    "            dropout_rate = best_params[\"dropout\"]\n",
    "            batch_size = best_params[\"batch_size\"]\n",
    "\n",
    "        with open(best_metrics_fp, \"r\") as f:\n",
    "            best_metrics = json.load(f)\n",
    "\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Iterate over folds\n",
    "        for fold_idx in [0, 1, 2, 3, 4]:\n",
    "            print(f\"Fold {fold_idx+1}\")\n",
    "\n",
    "            train_loader, val_loader, test_loader, _, input_dim, output_dim, y_mapping, scaler, _ = load_and_prepare_data(\n",
    "                dataset_key=dataset.lower(),\n",
    "                encoding_key=encoding,\n",
    "                multiclass=multiclass,\n",
    "                batch_size=batch_size,\n",
    "                cv=True,\n",
    "                cv_fold_index=fold_idx\n",
    "            )\n",
    "            print(f\"Dataset size: {len(train_loader.dataset)}, Batch size: {batch_size}, Num batches: {len(train_loader)}\")\n",
    "\n",
    "            model, criterion, optimizer = create_dynamic_neural_network(\n",
    "                input_dim=input_dim,\n",
    "                output_dim=output_dim,\n",
    "                multiclass=multiclass,\n",
    "                hidden_dims=hidden_dims,\n",
    "                optimizer=\"adam\",\n",
    "                lr=learning_rate,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            model = model.to(device)\n",
    "\n",
    "            # Train and evaluate\n",
    "            results, _ = train_adversarial(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                device=device,\n",
    "                num_epochs=10,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            print(\"\\n--- Evaluating on Test Set ---\")\n",
    "            test_metrics = compute_validation_metrics(\n",
    "                model=model,\n",
    "                val_loader=test_loader,\n",
    "                criterion=criterion,\n",
    "                device=device,\n",
    "            )\n",
    "            avg_test_loss, test_accuracy, test_precision, test_recall, test_f1 = test_metrics\n",
    "            print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}, Test F1 Score: {test_f1}. Precision: {test_precision}, Recall: {test_recall}\")  \n",
    "\n",
    "            # Collect metrics\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold_idx + 1,\n",
    "                \"accuracy\": results[\"val_accuracies\"][-1],\n",
    "                \"f1\": results[\"val_f1s\"][-1],\n",
    "                \"precision\": results[\"val_precisions\"][-1],\n",
    "                \"recall\": results[\"val_recalls\"][-1],\n",
    "                \"train_loss\": results[\"train_losses\"],\n",
    "                \"val_loss\": results[\"val_losses\"],\n",
    "            })\n",
    "\n",
    "        # Save fold metrics\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        with open(fold_metrics_fp, \"w\") as f:\n",
    "            json.dump(fold_metrics, f, indent=4)\n",
    "\n",
    "        # Average metrics\n",
    "        avg_metrics = {\n",
    "            metric: np.mean([fold[metric] for fold in fold_metrics]) for metric in [\"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "        }\n",
    "        avg_metrics[\"val_loss\"] = np.mean([fold[\"val_loss\"][-1] for fold in fold_metrics])  # Last validation loss\n",
    "\n",
    "        # Save average metrics\n",
    "        with open(avg_metrics_fp, \"w\") as f:\n",
    "            json.dump(avg_metrics, f, indent=4)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Average metrics for {dataset}: {avg_metrics}\")\n",
    "        print(f\"Best metrics for {dataset}: {best_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
